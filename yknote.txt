https://www.cnblogs.com/rossiXYZ/p/15897877.html
[源码解析] NVIDIA HugeCTR，GPU版本参数服务器 --(1)  2022-02-15 19:29  罗西的思考 ===》(tag: v3.4) 以前！
因此 HugeCTR 架构 2.1 版扩展为支持 Wide & Deep、DCN 和 DeepFM 等模型==》至少是2.1后的版本
主要是架构介绍, 很多重要的图，但是没有代码降解，所以这里记录！
0x00 摘要
0x01 背景
1.1 推荐系统中的点击率估计
1.2 点击率估算训练的挑战
0x02 HugeCtr
0x03 架构
3.1 CTR DL 模型
3.2 HugeCTR 架构
3.3 基于GPU的参数服务器
0x04 核心功能
4.1 模型并行训练
4.1.1 in-memory GPU hash table
4.1.2 Multi-slot embedding
4.1.3 具体实现
4.2 多节点训练
4.3 混合精度训练
4.4 SGD 优化器和学习率调度
4.5 嵌入训练缓存
4.6 HugeCTR 到 ONNX 转换器
4.7 分层参数服务器
4.8 异步多线程数据管道
4.9 灵活模型配置
0xFF 参考
0x00 摘要
本系列之中我们将会介绍 NVIDIA 出品的 HugeCTR，这是一个面向行业的推荐系统训练框架，
针对具有模型并行嵌入和数据并行密集网络的大规模 CTR 模型进行了优化。
本文以Introducing NVIDIA Merlin HugeCTR: A Training Framework Dedicated to Recommender Systems，
GitHub 源码文档 https://github.com/NVIDIA-Merlin/HugeCTR 的翻译为基础，并且结合源码进行分析。
其中借鉴了HugeCTR源码阅读 这篇大作，特此感谢，期望能在此篇大作基础之上，再丰富一下对HugeCTR的理解。

0x01 背景
我们将简要讨论 CTR 估计在现代推荐系统中的作用及其训练中的主要挑战。

1.1 推荐系统中的点击率估计
从在线广告和电子商务到流媒体服务，推荐系统无处不在，同时对服务提供商的收入产生巨大影响。
推荐系统找到给定用户最可点击的项目，然后对它们进行排名并向用户显示前 N 个项目。
为了实现这个目标，推荐系统首先必须估计特定用户点击项目的可能性。此任务通常称为 CTR 估计。

如何估算点击率？这里没有巫术，一般是获取包含 用户-物品 交互的富数据集，并使用它来训练 ML 模型。
数据集中的每条记录都可以包含来自用户（年龄、工作），商品（类型、价格）和用户商品点击（0 或 1）的特征。
例如，如果用户 A 从一系列书籍中购买或点击了几本传记，那么模型为传记分配高概率值是有意义的。
CTR 的系统结构大致如下：000-000.png
下图展示了CTR推理流程。000-001.png
图来自HugeCTR_Webinar

1.2 点击率估算训练的挑战
首先，推荐系统之中的特征有如下性质：高维，稀疏。
大规模推荐系统会面临用户和物品的频繁变化，因此识别用户点击背后的隐式特征交互至关重要，这样推荐系统可以提供更高质量的更通用的推荐。
例如，30 岁以下的已婚人士和孩子未满 2 岁的人可能倾向于购买高 ABV 的啤酒。
对这些隐式特征交互进行建模需要领域专家进行复杂的特征工程。
更糟糕的是，由于特征极其复杂且不直观，即使是人类专家也常常无法发现这些交互。
为了代替这种对专家的依赖，人们研究出了一些基于深度学习的方法，
例如 Wide & Deep，DeepFM 和 DLRM，这些模型可以捕获这些复杂的交互。

训练 CTR 估计模型的另一个挑战是用户和物品几乎每天都在变化，因此训练出来的模型其生命周期可能很短。
此外，由于数据集的大小的增加，维数和稀疏性因素，CTR 模型通常包含一个很大的嵌入表，其可能无法放入单个 GPU 甚至多个 GPU 的节点中。
因此，数据加载，嵌入表查找和 GPU 间通信可以占据模型训练时间的很大一部分。

这些因素，再加上缺乏用于 CTR 估算的标准化建模方法，通常导致服务在吞吐量和延迟方面经常只能达到次优性能。
所以在单个或多个 GPU 上完成模型的更快迭代训练是非常重要的。

0x02 HugeCtr
HugeCTR 是一个开源框架，用于在 NVIDIA GPU 上加速 CTR 估计模型的训练，
并针对 NVIDIA GPU 的性能进行了高度优化，同时允许用户以 JSON 格式自定义模型。
它是用 CUDA C++ 编写的，并且高度利用了 GPU 加速库，例如cuBLAS、cuDNN和NCCL。它最初是作为内部原型来评估 GPU 在 CTR 估计问题上的潜力，
但是其很快成为基于 GPU 的推荐系统的参考设计。
由于它自然而然地成为了专用于 CTR 估算的更通用的框架，
因此 NVIDIA 于 2019 年 9 月开源了其初始版本，以接受外部反馈，同时与一些客户保持互动。

HugeCTR 也是 NVIDIA Merlin的支柱，这是一个框架和生态系统，用于构建需要大量数据集进行训练的大规模推荐系统，
旨在促进推荐系统开发的所有阶段，并在 NVIDIA GPU 上加速。
000-003.png
图来自源码 https://github.com/NVIDIA-Merlin/Merlin
HugeCTR 在单个 NVIDIA V100 GPU 上的速度比 TensorFlow 在 40 核 CPU 节点上提高了 114 倍，
在同一个 V100 GPU 上实现了 TensorFlow 的 8.3 倍提高。
由于由线性模型和深度模型组成的混合模型已变得普遍，因此 HugeCTR 架构 2.1 版扩展为支持 Wide & Deep、DCN 和 DeepFM 等模型。
更新包括新的数据读取器，它可以同时读取连续和分类输入数据；以及新的层，包括因子分解机和交叉层。
为了实现更灵活的设计空间探索，还添加了 Dropout、L1/L2 正则化器等。

0x03 架构
3.1 CTR DL 模型
下图描绘了用于 CTR 估计的 DL 模型的步骤：
    按批次读取数据记录，每个记录都由高维、极其稀疏（或 categorical 类型）的特征组成。
    每个记录还可以包含密集的数字特征，这些特征可以直接馈送到全连接层。
    使用嵌入层将输入稀疏特征压缩为低维密集嵌入向量。
    例如，如果有 N 个稀疏特征，嵌入维度为 K，则嵌入表生成 N 个 K 维密集向量。
    使用前馈神经网络来估计点击率。
000-004.png
图上显示了一个典型的 CTR 模型，包括数据读取器、嵌入和全连接层。
图来自Introducing NVIDIA Merlin HugeCTR: A Training Framework Dedicated to Recommender Systems

3.2 HugeCTR 架构
HugeCTR 不仅支持 CTR DL 所有三个步骤，而且还增强了端到端的性能，比如：
    为了防止数据加载成为训练中的主要瓶颈，它实现了一个专用的数据读取器，该读取器是异步和多线程的。
    它将读取一组批处理数据记录，其中每条记录都由高维、极度稀疏或分类特征（categorical features）组成。
    每个记录还可以包含密集的数字特征（dense numerical features），这些特征可以直接馈送到全连接层。
    嵌入层用于将稀疏输入特征压缩为低维、密集的嵌入向量。共有三个 GPU 加速的嵌入阶段：
        表查找
        每个插槽（slot）内的权重规约。
        跨插槽的权重拼接（concatenation）。
    通过利用高效的 CUDA 优化技术和支持 CUDA 的库来支持前向和后向传播中的所有层，优化器和损失函数都是在 CUDA C++ 中实现的。
为了训练大规模 CTR 估计模型，HugeCTR 中的嵌入表是模型并行的，并分布在同构集群中的所有 GPU 上，该集群由多个节点组成。每个 GPU 都有自己的：
    前馈神经网络（数据并行）来估计点击率。
    哈希表使数据预处理更容易并启用动态插入。
所以，可以扩展到多个 GPU 和节点的HugtCTR的架构总结如下：000-005.png

3.3 基于GPU的参数服务器
HugeCTR 实现的是一个基于GPU的参数服务器，其将embedding层放到GPU之中，worker通过与参数服务器的交互来获取embedding。
000-006.png
图来自HugeCTR_Webinar

0x04 核心功能
在本节中，我们将介绍 HugeCTR 的关键特性，这些特性有助于其高性能和可用性。注意：多节点训练和混合精度训练可以同时使用。
4.1 模型并行训练
HugeCTR 原生支持模型并行和数据并行训练，使得在 GPU 上训练非常大的模型成为可能。
4.1.1 in-memory GPU hash table
在 CTR 估计中，嵌入（embedding）对于获得不错的模型精度几乎是必不可少的。
它通常会导致对内存容量和带宽的高需求以及相当数量的并行性。如果embedding分布在多个 GPU 或多个节点上，则通信开销也可能很大。
由于用户和物品数量庞大且不断增加，庞大的嵌入表在所难免。

为了克服这些挑战并实现更快的训练，HugeCTR实现了自己的嵌入层，其中包括一个 GPU 加速的哈希表，并利用NCCL 作为其 GPU 间通信原语。
哈希表的实现基于RAPIDS cuDF 的实现，RAPIDS cuDF 是来自 NVIDIA 的 GPU DataFrame 库。
cuDF GPU 哈希表可以比 Threading Building Blocks (TBB) 的 concurrent_hash_map 多出高达 35 倍的加速。

总之，HugeCTR 支持跨越同构计算集群中的多个 GPU 和多个节点的模型并行嵌入表。
嵌入的特征和类别可以分布在多个 GPU 和节点上。
例如，如果您有两个具有 8xA100 80GB GPU 的节点，则可以完全在 GPU 上训练大至 1TB 的模型。
通过使用嵌入训练缓存，您可以在相同节点上训练更大的模型。

4.1.2 Multi-slot embedding
嵌入表可以被分割成多个槽（或feature fields）。
在嵌入查找过程中，属于同一槽的稀疏特征输入在分别转换为相应的密集嵌入向量后，被简化为单个嵌入向量。
然后，来自不同槽的嵌入向量连接在一起。

多槽（multi-slot）嵌入通过以下方式提高了 GPU 间带宽利用率：
    当数据集中有很多特征时，它有助于将每个槽中有效特征的数量减少到可管理的程度。
    通过拼接不同插槽的输出，它减少了 GPU 之间的事务数量，从而促进了更高效的通信。
000-007.png
下图显示了操作序列和 GPU 间通信 ( all2all) 是如何发生的。
000-008.png
该图显示了一个跨越 4 个 GPU 的模型并行嵌入，以及它如何与这些 GPU 的神经网络进行交互。 它还显示了如何减少每个插槽的输入特征并跨两个插槽连接
图来自Introducing NVIDIA Merlin HugeCTR: A Training Framework Dedicated to Recommender Systems

多槽嵌入对线性模型也很有用，它基本上是特征的加权和，只需将槽数和嵌入维度都设置为 1 即可。有关更多信息，请参阅Wide & Deep 示例。

4.1.3 具体实现
为了在不同的嵌入上获得最佳性能，可以选择不同的嵌入层实现。这些实现中的每一个都针对不同的实际培训案例，例如：
    LocalizedSlotEmbeddingHash：
        同一个槽（特征域）中的特征会存储在一个GPU中，这就是为什么它被称为“本地化槽”，根据槽的索引号，不同的槽可能存储在不同的GPU中。
        LocalizedSlotEmbedding 针对每个embedding 小于 GPU 内存大小的实例进行了优化。
        由于在 LocalizedSlotEmbedding 中使用了每个插槽的局部规约（查完 embedding 得到向量之后，因为已经拿到了这个slot 的所有 embedding，
        可以做完pooling之后再做多GPU卡通信），而在 GPU 之间没有全局规约，
        因此 LocalizedSlotEmbedding 中的整体数据传输量远小于 DistributedSlotEmbedding。
        注意：确保输入数据集中没有任何重复的键。

    DistributedSlotEmbeddingHash：
        所有特征都存储于不同特征域/槽上，不管槽索引号是多少，这些特征都根据特征的索引号分布到不同的GPU上。
        这意味着同一插槽中的特征可能存储在不同的 GPU 中，这就是将其称为“分布式插槽”的原因。
        由于需要全局规约，所以 DistributedSlotEmbedding 适合 embedding 大于 GPU 内存大小的情况，
        因而 DistributedSlotEmbedding 在 GPU 之间有更多的内存交换。
        注意：确保输入数据集中没有任何重复的键。

    LocalizedSlotEmbeddingOneHot：
        一种特殊的 LocalizedSlotEmbedding，需要一个独热数据输入。每个特征字段也必须从零开始索引。
        例如，性别应该是0,1，而1,2 就不正确。

一定要注意，LocalizedSlotEmbeddingHash 和 DistributedSlotEmbeddingHash 的区别在于同一个槽（特征域）中的特征 是不是 会存储在同一个GPU中。
比如，有 2 张GPU卡，有4个slot。
    local 模式 ：GPU0 存 slot0 和 slot1，GPU1 存 slot2 和 slot3。
    distribute 模式 ：每个 GPU 都会存所有 slot 的一部分参数，通过哈希方法决定如何将一个参数分配到哪个 GPU 上。

4.2 多节点训练
多节点训练使得我们很容易训练任意大小的嵌入表。在多节点解决方案中，稀疏模型（称为嵌入层）分布在节点之间。
同时，密集模型（例如 DNN）是数据并行的，并且在每个 GPU 中都包含密集模型的副本（见下图）。
通过我们的实施，HugeCTR 利用 NCCL 进行高速和可扩展的节点间和节点内通信。
000-009.png
图来自源码。
要在多个节点上运行，HugeCTR 应该使用 OpenMPI 构建。
建议支持GPUDirect RDMA以获得高性能。有关更多信息，请参阅DCN 多节点训练样本。

4.3 混合精度训练
混合精度训练已成为在保持模型精度的同时实现进一步加速的常用技术，可以帮助我们改善和减少内存吞吐量占用。
在 HugeCTR 中，可以配置全连接层以利用 NVIDIA Volta 架构及其后续架构上的张量核心。
它们在内部使用 FP16 进行加速矩阵乘法，但其输入和输出仍为 FP32。

混合精度训练在这种模式下，TensorCores 被用于提高基于矩阵乘法的层的性能，
例如FullyConnectedLayer和InteractionLayer，在 Volta、Turing 和 Ampere 架构上。
对于包括嵌入在内的其他层，数据类型更改为 FP16，以便节省内存带宽和容量。
要启用混合精度模式，请在配置文件中指定 mix_precision 选项。当mixed_precision设定，完整的FP16管道将被触发。
将应用损失缩放以避免算术下溢（见图 ）。可以使用配置文件启用混合精度训练。
000-010.png
图 5：算术下溢 图来自源码。

4.4 SGD 优化器和学习率调度
学习率调度允许用户配置其超参数，包括以下内容：
    learning_rate：基础学习率。
    warmup_steps：用于预热的初始步骤数。
    decay_start：指定学习率衰减开始的时间。
    decay_steps：衰减期（逐步）。
图 6 说明了这些超参数如何与实际学习率相互作用。有关更多信息，请参阅Python 接口。
000-011.png
图 6：学习率调度 图来自源码。

4.5 嵌入训练缓存
嵌入训练缓存（Model Oversubscription）使您能够训练高达 TB 的大型模型。
它是通过在训练阶段以粗粒度、按需方式将超过 GPU 内存聚合容量的嵌入表的一个子集加载到 GPU 中来实现的。
要使用此功能，您需要将数据集拆分为多个子数据集，同时从中提取唯一键集（见图 7）。

此功能目前支持单节点和多节点训练。它支持所有嵌入类型，并且可以与Norm和Raw数据集格式一起使用。
我们修改了我们的criteo2hugectr工具以支持 Criteo 数据集的密钥集提取。
有关更多信息，请参阅我们的Python Jupyter Notebook，了解如何将此功能与 Criteo 数据集结合使用。

注意：Criteo 数据集是一个常见用例，但模型预取不限于此数据集。
000-012.png
Fig. 7: Preprocessing of dataset for model oversubscription 图来自源码。

4.6 HugeCTR 到 ONNX 转换器
HugeCTR to Open Neural Network Exchange (ONNX) 转换器是一个hugectr2onnxPython 包，可以将 HugeCTR 模型转换为 ONNX。
它可以提高 HugeCTR 与其他深度学习框架的兼容性，因为 ONNX 作为 AI 模型的开源格式。

使用我们的 HugeCTR Python API 进行训练后，您可以获得密集模型、稀疏模型和图形配置的文件，
这些文件在使用该hugectr2onnx.converter.convert方法时需要作为输入。
每个 HugeCTR 层将对应一个或多个 ONNX 算子，训练好的模型权重将作为初始化器加载到 ONNX 图中。
此外，您可以选择使用convert_embedding标志转换稀疏嵌入层。

4.7 分层参数服务器
HugeCTR 分层参数服务器 (POC) 上的本地 SSD 和 CPU 内存之间实现了分层存储机制。通过这种实现，嵌入表不再需要存储在本地 CPU 内存中。
添加了分布式 Redis 集群作为 CPU 缓存，以存储更大的嵌入表并直接与 GPU 嵌入缓存交互。
为了帮助 Redis 集群查找丢失的嵌入键，已实现本地 RocksDB 作为查询引擎来备份本地 SSD 上的完整嵌入表。

4.8 异步多线程数据管道
如果没有高效的数据管道，即使向前和向后传播以光速运行，其效果也如同到达机场的时间远长于飞行时间。
另外，当数据集很大并且经常变化时，将其拆分为多个文件是非常合理的。

为了有效地把数据获取这个长延迟隐藏起来，HugeCTR 有一个多线程数据读取器，其可以将数据获取与实际模型训练重叠起来。
如下图所示，DataReader是一个façade，由多个并行工作器和一个收集器组成。

每个工作器每次从其分配到的数据集文件中读取一个批次。收集器会将收集到的数据记录分发到多个 GPU。
所有的工作人员、收集器和模型训练作为不同的线程在 CPU 上同时运行。
000-013.png
该图显示了四个数据读取器如何将数据从磁盘读取到主机内存，一个收集器如何读取其中之一以提供给模型训练管道。
Figure 4. HugeCTR multithreaded data reader.
图来自Introducing NVIDIA Merlin HugeCTR: A Training Framework Dedicated to Recommender Systems

下图 000-014.png
显示了 HugeCTR 流水线如何把 "数据从磁盘读取到 CPU 内存的数据"，
"从 CPU 到 GPU 的数据传输"以及"在 GPU 上跨不同批次的实际训练"这三个阶段重叠起来。
此图显示了“读取文件”、“复制到 GPU”和“训练”阶段如何重叠三个批次以提高 GPU 资源利用率。
图来自Introducing NVIDIA Merlin HugeCTR: A Training Framework Dedicated to Recommender Systems

4.9 灵活模型配置
尽管 CTR 模型之间存在一些共性，但它们的细节（包括超参数）可能有所不同。
为了实现模型的灵活定制，HugeCTR 允许以 JSON 格式直观地配置模型。

例如，要描述如下图所示的混合模型，您可以编写如图 (b) 中抽象所示的“layers”子句。
您可以有多个嵌入，您还可以指定批处理大小、优化器、数据路径等。
在同一个配置文件中，您也可以指定用于训练的 GPU 数量和数量。
有关更多信息，请参阅HugeCTR 用户指南和示例配置文件。
000-015.png  == Figure 6. A
000-016.png  == Figure 6. B
Figure 6. A hybrid model with two embeddings and two different types of inputs.
(a) An example mode expressible by HugeCTR.
(b) The corresponding config. A lot of details are omitted for simplicity.
图来自Introducing NVIDIA Merlin HugeCTR: A Training Framework Dedicated to Recommender Systems

0xFF 参考
Introducing NVIDIA Merlin HugeCTR: A Training Framework Dedicated to Recommender Systems
Announcing NVIDIA Merlin: An Application Framework for Deep Recommender Systems
https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/
https://developer.nvidia.com/blog/accelerating-recommender-systems-training-with-nvidia-merlin-open-beta/
HugeCTR源码阅读
embedding层如何反向传播
https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html
https://info.nvidia.com/235418-ondemand.html
HugeCTR_Webinar
https://www.cnblogs.com/futurehau/p/6181008.html
https://www.cnblogs.com/rossiXYZ/p/15902100.html


=============================================================================================================================================================
=============================================================================================================================================================
=============================================================================================================================================================


[源码解析] NVIDIA HugeCTR，GPU版本参数服务器--- (2)
solver_config = hugectr.solver_parser_helper(batchsize = 16384 == 只有v2.3, v3.0， v3.0.1
std::vector<std::shared_ptr<IEmbedding>> embeddings_ 最先v3.0
 Session(const SolverParser& solver_config, const std::string& config_file)  最先v3.1 ====后续搜索显示都有，基本确定！！！
 这里CSR是嵌入层依赖的数据格式，我们下文会分析。
 001-002.png
0xFF 参考
https://developer.nvidia.com/blog/introducing-merlin-hugectr-training-framework-dedicated-to-recommender-systems/

https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/

https://developer.nvidia.com/blog/accelerating-recommender-systems-training-with-nvidia-merlin-open-beta/

HugeCTR源码阅读

embedding层如何反向传播

https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html

HugeCTR_Webinar

 =============================================================================================================================================================
 =============================================================================================================================================================
 =============================================================================================================================================================

https://www.cnblogs.com/rossiXYZ/p/15905658.html
[源码解析] NVIDIA HugeCTR，GPU版本参数服务器---(3)
 0x02 数据集
 HugeCTR 目前支持三种数据集格式，即Norm、Raw和Parquet，具体格式参见如下：
002-001.png
 Fig. 1: (a) Norm (b) Raw (c) Parquet Dataset Formats
2.1 Norm
 为了最大化数据加载性能并最小化存储，Norm 数据集格式由一组二进制数据文件和一个 ASCII 格式的文件列表组成。
 模型文件应指定训练和测试（评估）集的文件名，样本中的元素（键）最大数目和标签维度，具体如图 1（a）所示。
..
4.2.4 小结
至此，Tensor的逻辑拓展一下：
    TensorBufferImpl 的 buffer 是GeneralBuffer2；
    GeneralBuffer2 的 ptr 是由CudaAllocator在GPU之中分配的；GeneralBuffer2 可以认为是一个对大段内存的统一封装，在其上可以有若干Tensor。这些Tensor先reserve内存，然后统一分配。
    TensorBufferImpl 的 offset_ 就指向了 GeneralBuffer2 的 ptr 之中具体的某一个内存偏移；
    BufferBlockImpl 用来实现一个连续的Tensor内存。
002-006.jpg

如果还有另外一个 Tensor2，则其 TensorBufferImpl.offset 会指向 GPU内存的另外一个offset，
比如下面有两个张量，Tensor 1 和 Tensor 2。
002-007.jpg

0xFF 参考
https://developer.nvidia.com/blog/introducing-merlin-hugectr-training-framework-dedicated-to-recommender-systems/

https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/

https://developer.nvidia.com/blog/accelerating-recommender-systems-training-with-nvidia-merlin-open-beta/

HugeCTR源码阅读

embedding层如何反向传播

https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html

稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB

 =============================================================================================================================================================
 =============================================================================================================================================================
 =============================================================================================================================================================

https://www.cnblogs.com/rossiXYZ/p/15916979.html
[源码解析] NVIDIA HugeCTR，GPU版本参数服务器--- (4)
我们再看看3.2.1版本的代码，也选取了部分成员变量  === 》 从本文看，应该是这个版本！！！

0x06 总结
具体逻辑如下，本章节之中，各个buffer之间拷贝，是依据其状态是 ReadyForRead 和 ReadyForWrite 来完成的。
最终sparse 参数的embedding是在DataReaderOutput，即后续 GPU 上的计算是从output开始的。
003-012.jpg

4.7.4 小结
我们总结逻辑如下，线程一直调用 data_reader_thread_func_ 来循环读取：
003-007.jpg
另外一个逻辑视角是：
    多线程调用 data_reader_thread_func_，其使用 read_a_batch 从数据文件之中读取数据解析为CSR。每一个embedding层 对应一个CSR。
    CSR 被放入 DataReaderWorker 的 host_sparse_buffer_。
    随着batch不断读取，CSR 行数在不断增加，每一个slot对应了一行，所以一个batch的行数就是 batch_size * slot_num。
    使用 cudaMemcpyAsync 把CSR从 host_sparse_buffer_ 拷贝到ThreadBuffer（位于GPU）。ThreadBuffer是 SparseTensor 类型了。
    目前CSR数据就在 GPU 之上了。
这里简化了多GPU，多worker 的情况。
003-008.jpg

0xFF 参考
https://developer.nvidia.com/blog/introducing-merlin-hugectr-training-framework-dedicated-to-recommender-systems/

https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/

https://developer.nvidia.com/blog/accelerating-recommender-systems-training-with-nvidia-merlin-open-beta/

HugeCTR源码阅读

embedding层如何反向传播

https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html

稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB

 =============================================================================================================================================================
 =============================================================================================================================================================
 =============================================================================================================================================================



https://www.cnblogs.com/rossiXYZ/p/15924047.html
[源码解析] NVIDIA HugeCTR，GPU版本参数服务器--- (5) 嵌入式hash表


在这篇文章中，我们介绍了 HugeCTR，这是一个面向行业的推荐系统训练框架，针对具有模型并行嵌入和数据并行密集网络的大规模 CTR 模型进行了优化。
其中借鉴了HugeCTR源码阅读 这篇大作，特此感谢。

0x01 前文回顾
在前文，我们已经完成了对HugeCTR流水线的分析，接下来就要看嵌入层的实现，这部分是HugeCTR的精华所在。
嵌入在现代基于深度学习的推荐架构中发挥着关键作用，其为数十亿实体（用户、产品及其特征）编码个体信息。
随着数据量的增加，嵌入表的大小也在增加，现在跨越多个 GB 到 TB。
因为其巨大的嵌入表和稀疏访问模式可能跨越多个 GPU，所以训练这种类型的 DL 系统存在独特的挑战。

HugeCTR 就实现了一种优化的嵌入实现，其性能比其他框架的嵌入层高 8 倍。
这种优化的实现也可作为 TensorFlow 插件使用，可与 TensorFlow 无缝协作，并作为 TensorFlow 原生嵌入层的便捷替代品。

0x02 Embedding
2.1 概念
我们先简要介绍一下embedding的概念。嵌入（embedding）是一种机器学习技术，
被用来把一个id/category特征自动转换为待优化的特征向量，这样可以把算法从精确匹配拓展到模糊匹配，从而提高算法的拓展能力。
从另一个角度看，嵌入表是一种特定类型的key-value存储，键是用于唯一标识对象的 ID，值是实数向量。

Embedding技术在 NLP 中很流行，用来把单词表示密集数值向量，具有相似含义的单词具有相似的嵌入向量。

另外，Embedding 还有一个优势就是把数据从高维转换为低维。

2.1.1 One-hot 编码
作为比对，我们先看看 One-hot 编码。One-hot编码就是保证每个样本中的单个特征只有1位处于状态1，其他的都是0。具体编码举例如下，把语料库中，杭州、上海、宁波、北京每个都对应一个向量，向量中只有一个值为1，其余都为0。

杭州 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0]
上海 [0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0]
宁波 [0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0]
北京 [0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0]
其缺点是：
    向量的维度会随着词的数量增大而增大；如果将世界所有城市名称对应的向量组成一个矩阵的话，那这个矩阵因为过于稀疏则会造成维度灾难。
    因为城市编码是随机的，所以向量之间相互独立，无法表示词汇之间在语义层面上的相关信息。
所以，人们想对独热编码做如下改进：
    将vector每一个元素由整型改为浮点型，从整型变为整个实数范围的表示；
    把原始稀疏向量转化为低维度的连续值，也就是稠密向量。可以认为是将原来稀疏的巨大维度压缩嵌入到一个更小维度的空间。并且其中意思相近的词将被映射到这个向量空间中相近的位置。
    简单说，就是寻找一个空间映射把高维词向量嵌入到一个低维空间。

2.1.2 分布式表示
分布式表示（Distributed Representation）基本思想是将每个词表达成 n 维稠密、连续的实数向量。
而实数向量之间的关系可以代表词语之间的相似度，比如向量的夹角cosine或者欧氏距离。
用词汇举例，独热编码相当于对词进行编码，而分布式表示则是将词从稀疏的大维度压缩嵌入到较低维度的向量空间中。

分布式表示最大的贡献就是让相关或者相似的词在距离上更接近了。
分布式表示相较于One-hot方式另一个区别是维数下降很多，对于一个100万的词表，我们可以用100维的实数向量来表示一个词，而One-hot得要100W万来编码。
比如杭州、上海、宁波、北京，广州，深圳，沈阳，西安，洛阳 这九个城市 采用 one-hot 编码如下：

杭州 [1,0,0,0,0,0,0,1,0]
上海 [0,1,0,0,0,0,0,0,0]
宁波 [0,0,1,0,0,0,0,0,0]
北京 [0,0,0,1,0,0,0,0,0]
广州 [0,0,0,0,1,0,0,0,0]
深圳 [0,0,0,0,0,1,0,0,0]
沈阳 [0,0,0,0,0,0,1,0,0]
西安 [0,0,0,0,0,0,0,1,0]
洛阳 [0,0,0,0,0,0,0,0,1]
但是太稀疏，占用太大内存，所以弄一个稠密矩阵，也就是 Embedding Table 如下：
....
如果要查找杭州，上海：
....
利用矩阵乘法，可以得到：
......

        004-clip-01.png

这样就把两个 1 x 9 的高维度，离散，稀疏向量，压缩到 两个 1 x 3 的低维稠密向量。
这里把 One-Hot 向量中 “1”的位置叫做sparseID，就是一个编号。
这个独热向量和嵌入表的矩阵乘法就等于利用sparseID进行的一次查表过程，就是依据杭州，上海的sparseID（0，1），从嵌入表中取出对应的向量（第0行、第1行）。
这样就把高维变成了低维。

2.1.3 推荐领域
在推荐系统领域，Embedding将每个感兴趣的对象（用户、产品、类别等）表示为一个密集的数值向量。
最简单的推荐系统基于用户和产品：您应该向用户推荐哪些产品？您有用户 ID 和产品 ID作为key。
对应的value则是用户和产品，因此您使用两个嵌入表。
004-001.png
图1. 嵌入表是稀疏类别的密集表示。 每个类别由一个向量表示，这里嵌入维度是4。来自 Using Neural Networks for Your Recommender System。

2.2 Lookup
从上图能够看到另外一个概念：lookup，我们接下来就分析一下。
Embeddings层的神经元个数是由embedding vector和field_size共同确定，即神经元的个数为embedding vector * field_size。
dense vector是embeding层的输出的水平拼接。由于输入特征one-hot编码，所以embedding vector也就是输入层到Dense Embeddings层的权重，也就是全连接层的权重。

Embedding 权重矩阵可以是一个 [item_size, embedding_size] 的稠密矩阵，item_size是需要embedding的物品个数，
embedding_size是映射的向量长度，或者说矩阵的大小是：特征数量 * 嵌入维度。
Embedding 权重矩阵的每一行对应输入的一个维度特征（one-hot之后的维度）。用户可以用一个index表示选择了哪个特征。

Embedding_lookup 就是如何从Embedding 权重矩阵获取到一个超高维输入对应的embedding向量的方法，
embedding就是权重本身。Embedding_lookup 实际上是由矩阵相乘实现的 V = WX + b，因为输入 X 是One-Hot编码，
所以和矩阵相乘相当于是取出权重矩阵中对应的那一行，看起来像是在查一个索引表，所以叫做 lookup。
Embedding 本质上可以看做是一个全连接层。比如：
....  004-clip-02.png
嵌入层这些权重是通过神经网络自己学习到的，实际上，权重矩阵一般是随机初始化的，是需要优化的变量。训练神经网络时，每个Embedding向量都会得到更新，即在不断升维和降维的过程中，找到最适合的维度。因此，embedding_lookup 这里还需要完成反向传播，即自动求导和对权重矩阵的更新。

2.3 嵌入层
嵌入层是现代深度学习推荐系统的关键模块，其通常位于输入层之后，在特征交互和密集层之前。
嵌入层就像深度神经网络的其他层一样，是从数据和端到端训练中学习得到的。我们接下来看看如何使用嵌入层。

2.3.1 点积
我们可以计算用户嵌入和项目嵌入之间的点积以获得最终分数，即用户与项目交互的可能性。
您可以应用 sigmoid 激活函数作为将输出转换为 0 到 1 之间的概率的最后一步。
dotproduct:u.v=∑ai.bi
004-002.png
图 2. 具有两个嵌入表和点积输出的神经网络。来自 Using Neural Networks for Your Recommender System。
此方法等效于矩阵分解或交替最小二乘法 (ALS)。

2.3.2 全连接层
如果使用多个非线性层构建一个深层结构，则神经网络性能会更佳。您可以通过使用 ReLU 激活将嵌入层的输出馈送到多个完全连接的层来扩展先前的模型。
这里在设计上的一个选择点是：如何组合两个嵌入向量。您可以只连接嵌入向量，也可以将向量按元素相乘，类似于点积。点积输出后跟着多个隐藏层。
004-003.png
图 3. 具有两个嵌入表和多个全连接层的神经网络，将用户和产品嵌入进行concatenate 或者进行元素级别（element-wise）相乘。
多个隐藏层可以处理结果向量。来自 Using Neural Networks for Your Recommender System。

2.3.3 元数据信息
到目前为止，我们只使用了用户 ID 和产品 ID 作为输入，但我们通常有更多可用信息。
比如用户的其他信息可以是性别、年龄、城市（地址）、自上次访问以来的时间或用于付款的信用卡。
一件商品通常有过去 7 天内售出的品牌、价格、类别或数量。这些辅助信息可以帮助模型更好地泛化。
我们可以修改神经网络以使用附加特征作为输入。
004-004.png
图 4. 具有元信息和多个全连接层的神经网络，我们向神经网络架构添加更多信息，比如可以添加例如城市、年龄、分支机构、类别和价格等辅助信息。
来自 Using Neural Networks for Your Recommender System。

至此，我们可知，基于嵌入层我们可以得到一个基础的推荐系统网络。

2.3.4 经典架构
接下来，我们看看 2016 年 Google 的 Wide and Deep 和 Facebook 2019 年的 DLRM。

2.3.4.1 Google’s Wide and Deep
Google 的 Wide 和 Deep 包含两个组件：
    与之前的神经网络相比，Wide部分是新组件，它是输入特征的线性组合，具有类似线性/逻辑回归的逻辑。
    Deep部分的作用是：把高维，稀疏的类别特征通过嵌入层处理为低维稠密向量，并将输出与连续值特征连接起来。连接之后的向量传递到MLP层。
两个部分的输出通过加权相加合并在一起得到最终预测值。

对于推荐系统模型来说，理想状态是同时具有记忆能力和泛化能力。
    泛化能力指的是模型对未观察到的特征或者罕见特征可以做出预测。
        比如通过学习得到一个规则是：有翅膀会飞，所以看见喜鹊就可以推断它会飞，这就是泛化能力。
        Deep部分 通过基于用户/物品特征进行归纳以提供优秀的泛化能力。
    记忆能力指的是模型可以记住大量的历史行为特征，然后从历史数据之中学习到特征的共性，而且可以把这些共性作为推荐依据。
        泛化能力会有疏漏，比如企鹅不会飞。

        Wide部分 就可以通过历史等记忆信息来提供优秀的记忆能力，纠正例外情况。逻辑回归这样的简单模型如果发现一个“强特征”，
        就会在训练时候把其相应的权重调整得非常大，从而实现了对这个特征的记忆，这就是所谓的模型“记忆能力”。

        我们给出论文（https://arxiv.org/abs/1606.07792）之中的架构图，可以看到，wide部分选择了两个特征：用户安装的app，曝光app，然后加交叉得到了一个特征，
        比如用户已经安装了netflix，而且在应用商店中曾经看过pandora应用，那么交叉特征：AND(user_installed_app=netflix, impression_app=pandora)就会被设置为1。
        这就是希望模型可以记住这样的规则：“如果用户已经安装了应用 A，是否会安装 B”。
因此，Wide and Deep 模型兼具逻辑回归和深度神经网络的优点，可以记忆大量历史行为，又拥有强大的表达能力。`
004-005.png
在HugeCTR之中，可以看到是通过如下方式来组织模型的。
004-006.jpg

2.3.4.2 Facebook 的 DLRM
Facebook 的 DLRM（Deep Learning Recommendation Model） 与带有元数据的神经网络架构具有相似的结构，但有一些区别。
数据集可以包含多个分类特征。DLRM 要求所有分类输入都通过具有相同维度的嵌入层馈送。

接下来，连续输入被串联并通过多个完全连接的层馈送，称为底层多层感知器 (MLP)。
底部 MLP 的最后一层与嵌入层向量具有相同的维度。

DLRM 使用新的组合层。它在所有嵌入向量对和底部 MLP 输出之间应用逐元素（element-wise）乘法。
这就是每个向量具有相同维度的原因。生成的向量被连接起来并且发送给另一组全连接层（顶部 MLP）。
004-007.jpg
图 5. Wide and Deep 架构在左侧可视化，DLRM 架构在右侧。
本节图片来自 Using Neural Networks for Your Recommender System。

2.4 推荐系统的嵌入层
因为CTR领域之中，特征的特点是高维，稀疏，所以对于离散特征，一般使用one-hot编码。
但是将One-hot类型的特征输入到DNN中，会导致网络参数太多，比如输入层有1000万个节点，隐层有500节点，则参数有50亿个。

所以人们增加了一个Embedding层用于降低维度，这样就对单个特征的稀疏向量进行紧凑化处理。但是有时还是不奏效，人们也可以将特征分为不同的field。

2.4.1 特色
和与其他类型 DL 模型相比，DL 推荐模型的嵌入层是比较特殊的：它们为模型贡献了大量参数，但几乎不需要计算，而计算密集型denser layers的参数数量则要少得多。

举一个具体的例子：原始的Wide and Deep模型有几个大小为[1024,512,256]的dense layers，
因此这些dense layers只有几百万个参数，而其嵌入层可以有数十亿个条目，以及数十亿个参数。
这与例如在 NLP 领域流行的 BERT 模型架构形成对比，BERT的嵌入层只有数万个条目，总计数百万个参数，但其稠密的前馈和注意力层则由数亿个参数组成。
这种差异还导致另一个结果：与其他类型的 DL 模型相比，DL 推荐网络输入数据的每字节计算量通常要小得多。

2.4.2 优化嵌入重要性
对于推荐系统，嵌入层的优化十分重要。要理解为什么嵌入层和相关操作的优化很重要，首先要看看推荐系统嵌入层训练所遇到的挑战：数据量和速度。

2.4.2.1 数据量
随着在线平台和服务获得数亿甚至数十亿用户，并且提供的独特产品数量达到数十亿，嵌入表的规模越来越大也就不足为奇了。

据报道，Instagram 一直致力于开发大小达到 10 TB 的推荐模型。同样，百度的一个广告排名模型，也达到了 10 TB 的境界。
在整个行业中，数百 GB 到 TB 的模型正变得越来越流行，例如Pinterest 的 4-TB 模型和Google 的 1.2-TB 模型。
因此，在单个计算节点上拟合 TB 级模型是一项重大挑战，更不用说在单个计算加速器（比如GPU）了。NVIDIA A100 GPU 只是配备了 80 GB 的 HBM。

2.4.2.2 访问速度
训练推荐系统本质上是一项内存带宽密集型任务。这是因为每个训练样本或批次通常涉及嵌入表中的少量实体。
必须检索这些条目才能计算前向传递，然后在后向传递中更新。

CPU 主存容量大但带宽有限，高端机型通常在几十 GB/s 范围内。
另一方面，GPU 的内存容量有限，但带宽很高。NVIDIA A100 80-GB GPU 提供 2 TB/s 的内存带宽。

2.4.3 解决方案
针对这些挑战，人们已经找到了一些解决方案，但是都存在一些问题，比如：
    将整个嵌入表保存在主存储器上解决了大小问题。然而，它通常会导致训练吞吐量极慢，而新数据的数量和速度往往使这种吞吐量相形见绌，从而导致系统无法及时重新训练。
    或者，可以把嵌入层分布在多个 GPU 和多个节点上，但这样却陷入通信瓶颈，导致 GPU 计算利用率不足，训练性能与纯 CPU 训练不相上下。
因此，嵌入层是推荐系统的主要瓶颈之一。优化嵌入层是解锁 GPU 高计算吞吐量的关键。

0x03 DeepFM
因为我们要用DeepFM作为示例，所以需要介绍一下基本内容。
我们选择 IJCAI 2017 的论文 DeepFM: A Factorization-Machine based Neural Network for CTR Prediction 的内容做分析。

3.1 CTR特点
CTR预估数据有如下特点：
    输入的数据有类别型和连续型。类别型数据会编码成one-hot，连续型数据可以先离散化再变吗为one-hot，也可以保留原值。
    数据的维度非常高。
    数据非常稀疏。
    特征按照Field分组。
CTR预估重点在于学习组合特征。Google论文研究结论为：高阶和低阶的组合特征都非常重要，应该同时学习到这两种组合特征。所以关键点是如何高效的提取这些组合特征。

3.2 DeepFM
DeepFM将Google的wide & deep模型进行改进：
    将Wide & Deep 部分的wide部分由 人工特征工程 + LR 转换为FM模型，FM提取低阶组合特征，Deep提取高阶组合特征，这样避开了人工特征工程，提高了模型的泛化能力；
    FM模型和Deep部分共享Embedding，使得DeepFM成为一种端到端的模型，提高了模型的训练效率，不但训练更快而且更准确。
具体模型架构如下： 004-008.png

0x04 HugeCTR嵌入层
为了克服嵌入挑战并实现更快的训练，HugeCTR 实现了自己的嵌入层，其中包括一个 GPU 加速的哈希表、以节省内存的方式实现的高效稀疏优化器以及各种嵌入分布策略。
它利用NCCL作为其 GPU 间通信原语。

4.1 哈希表
哈希表的实现基于RAPIDS cuDF，它是一个 GPU DataFrame 库，是 NVIDIA 的 RAPIDS 数据科学平台的一部分。
cuDF GPU 哈希表可以比基于 Threading Building Blocks (TBB) 实现的 concurrent_hash_map 更快，而且达到 35 倍的加速。

4.2 模型并行
HugeCTR 提供了一个模型并行的嵌入表，其分布在集群中的所有 GPU 上，集群由多个节点和多个 GPU 组成。另一方面，密集层采用数据并行性，每个 GPU 上有一个副本。
考虑到可扩展性，HugeCTR 默认支持嵌入层的模型并行性。
004-009.png
图 6. HugeCTR 模型和数据并行架构。来自 https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/。

4.3 通信
HugeCTR 使用NCCL 完成了高速和可扩展的节点间和节点内通信。对于有很多输入特征的情况，HugeCTR嵌入表可以分割成多个槽。
属于同一个槽的特征被独立转换为对应的嵌入向量，然后被规约为单个嵌入向量。这允许用户将每个插槽中的有效功能的数量有效地减少到可管理的程度。

4.4 印证
我们可以从 https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/DLRM#hybrid-parallel-multi-gpu-with-all-2-all-communication 之中看到混合并行的思路，
可以和HugeCTR印证（因为我们使用DeepFM来剖析，所以DLRM只用于印证）。

4.4.1 DLRM
在DLRM之中，为了处理类别数据，嵌入层将每个类别映射到密集表示，然后再输入多层感知器 (MLP)。数值特征则可以直接输入 MLP。
在下一级，通过取所有嵌入向量对和处理过的密集特征之间的点积，显式计算不同特征的二阶交互。
这些成对的交互被馈送到顶级 MLP 以计算用户和产品对之间交互的可能性。

与其他基于 DL 的推荐方法相比，DLRM 在两个方面有所不同。
首先，它明确计算了特征交互，同时将交互顺序限制为成对交互（pairwise interactions）。
其次，DLRM 将每个嵌入的特征向量（对应于类别特征）视为一个单元，而其他方法（例如 Deep and Cross）将特征向量中的每个元素视为一个新单元，
这样会会产生不同的交叉项。这些设计选择有助于降低计算和内存成本，同时也可以提供相当的准确性。
004-010.png
图来自 https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/。

4.4.2 混合并行
许多推荐模型包含非常大的嵌入表。因此，该模型往往太大，无法装入单个设备。
这可以通过使用CPU或其他GPU作为 "内存捐赠者(memory donors)"，以模型并行的方式进行训练而轻松解决。
然而，这种方法是次优的，因为 "内存捐赠者 "设备的计算没有被利用。

针对 DLRM，我们对模型的底层部分使用模型并行方法（嵌入表+底层MLP），而对模型的顶层部分使用通常的数据并行方法（Dot Interaction + Top MLP）。
这样，我们可以训练比通常适合单个GPU的模型大得多的模型，同时通过使用多个GPU使训练更快。我们称这种方法为混合并行。
004-011.png
图来自 https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/DLRM#hybrid-parallel-multi-gpu-with-all-2-all-communication 。

4.5 使用
我们可以采用如下两种方式来使用 hugeCTR 嵌入层：
    将原生 NVIDIA Merlin HugeCTR 框架用于训练和推理工作。
    使用 NVIDIA Merlin HugeCTR TensorFlow 插件，该插件旨在与 TensorFlow 无缝协作。

5.1.2.3 TensorFlow
我们可以从TF源码注释里面看到pooling的一些使用。

tensorflow/python/ops/embedding_ops.py 是关于embedding的使用。

    combiner: A string specifying the reduction op. Currently "mean", "sqrtn"
      and "sum" are supported. "sum" computes the weighted sum of the embedding
      results for each row. "mean" is the weighted sum divided by the total
      weight. "sqrtn" is the weighted sum divided by the square root of the sum
      of the squares of the weights. Defaults to `mean`.
tensorflow/python/feature_column/feature_column.py 是关于feature column的使用。

sparse_combiner: A string specifying how to reduce if a categorical column
  is multivalent. Except `numeric_column`, almost all columns passed to
  `linear_model` are considered as categorical columns.  It combines each
  categorical column independently. Currently "mean", "sqrtn" and "sum" are
  supported, with "sum" the default for linear model. "sqrtn" often achieves
  good accuracy, in particular with bag-of-words columns.
    * "sum": do not normalize features in the column
    * "mean": do l1 normalization on features in the column
    * "sqrtn": do l2 normalization on features in the column
在：tensorflow/lite/kernels/embedding_lookup_sparse.cc 的注释有直接从嵌入表之中look up的使用。

// Op that looks up items from a sparse tensor in an embedding matrix.
// The sparse lookup tensor is represented by three individual tensors: lookup,
// indices, and dense_shape. The representation assume that the corresponding
// dense tensor would satisfy:
//   * dense.shape = dense_shape
//   * dense[tuple(indices[i])] = lookup[i]
//
// By convention, indices should be sorted.
//
// Options:
//   combiner: The reduction op (SUM, MEAN, SQRTN).
//     * SUM computes the weighted sum of the embedding results.
//     * MEAN is the weighted sum divided by the total weight.
//     * SQRTN is the weighted sum divided by the square root of the sum of the
//       squares of the weights.
//
// Input:
//     Tensor[0]: Ids to lookup, dim.size == 1, int32.
//     Tensor[1]: Indices, int32.
//     Tensor[2]: Dense shape, int32.
//     Tensor[3]: Weights to use for aggregation, float.
//     Tensor[4]: Params, a matrix of multi-dimensional items,
//                dim.size >= 2, float.
//
// Output:
//   A (dense) tensor representing the combined embeddings for the sparse ids.
//   For each row in the sparse tensor represented by (lookup, indices, shape)
//   the op looks up the embeddings for all ids in that row, multiplies them by
//   the corresponding weight, and combines these embeddings as specified in the
//   last dimension.
//
//   Output.dim = [l0, ... , ln-1, e1, ..., em]
//   Where dense_shape == [l0, ..., ln] and Tensor[4].dim == [e0, e1, ..., em]
//
//   For instance, if params is a 10x20 matrix and ids, weights are:
//
//   [0, 0]: id 1, weight 2.0
//   [0, 1]: id 3, weight 0.5
//   [1, 0]: id 0, weight 1.0
//   [2, 3]: id 1, weight 3.0
//
//   with combiner=MEAN, then the output will be a (3, 20) tensor where:
//
//   output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)
//   output[1, :] = (params[0, :] * 1.0) / 1.0
//   output[2, :] = (params[1, :] * 3.0) / 3.0
//
//   When indices are out of bound, the op will not succeed.
另外，其他框架/模型实现也有使用加权平均（比如使用Attention），或者加入时序信息。

5.2 构建
这是一个比较复杂的过程，从前文我们知道，DataReader最后把各种输入都拷贝到了其成员变量 output_ 之上。
004-017.jpg
那么，嵌入层是如何利用到 output_ 中的sparse特征的呢？我们需要一步一步来看。

0xFF 参考
Using Neural Networks for Your Recommender System

Accelerating Embedding with the HugeCTR TensorFlow Embedding Plugin

https://developer.nvidia.com/blog/introducing-merlin-hugectr-training-framework-dedicated-to-recommender-systems/

https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/

https://developer.nvidia.com/blog/accelerating-recommender-systems-training-with-nvidia-merlin-open-beta/

NVIDIA Merlin HugeCTR 简介：专用于推荐系统的培训框架

HugeCTR源码阅读

embedding层如何反向传播

https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html

稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB

无中生有：论推荐算法中的Embedding思想

tf.nn.embedding_lookup函数原理

求通俗讲解下tensorflow的embedding_lookup接口的意思？

【技术干货】聊聊在大厂推荐场景中embedding都是怎么做的

ctr预估算法对于序列特征embedding可否做拼接，输入MLP?与pooling

推荐系统中的深度匹配模型

土法炮制：Embedding 层是如何实现的？

不等距双杆模型_搜索中的深度匹配模型（下）

深度特征 快牛策略关于高低层特征融合

[深度学习] DeepFM 介绍与Pytorch代码解释

deepFM in pytorch

推荐算法之7——DeepFM模型

DeepFM 参数理解（二）

推荐系统遇上深度学习(三)--DeepFM模型理论和实践

[深度学习] DeepFM 介绍与Pytorch代码解释

https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html

带你认识大模型训练关键算法：分布式训练Allreduce算法

FLEN: Leveraging Field for Scalable CTR Prediction

 =============================================================================================================================================================
 =============================================================================================================================================================
 =============================================================================================================================================================


https://www.cnblogs.com/rossiXYZ/p/15928428.html
[源码解析] NVIDIA HugeCTR，GPU版本参数服务器--- (6) --- Distributed hash表
0xFF 参考
https://developer.nvidia.com/blog/introducing-merlin-hugectr-training-framework-dedicated-to-recommender-systems/

https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/

https://developer.nvidia.com/blog/accelerating-recommender-systems-training-with-nvidia-merlin-open-beta/

HugeCTR源码阅读

embedding层如何反向传播

https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html

稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB

无中生有：论推荐算法中的Embedding思想

tf.nn.embedding_lookup函数原理

求通俗讲解下tensorflow的embedding_lookup接口的意思？

【技术干货】聊聊在大厂推荐场景中embedding都是怎么做的

ctr预估算法对于序列特征embedding可否做拼接，输入MLP?与pooling

推荐系统中的深度匹配模型

土法炮制：Embedding 层是如何实现的？

不等距双杆模型_搜索中的深度匹配模型（下）

深度特征 快牛策略关于高低层特征融合

[深度学习] DeepFM 介绍与Pytorch代码解释

deepFM in pytorch

推荐算法之7——DeepFM模型

DeepFM 参数理解（二）

推荐系统遇上深度学习(三)--DeepFM模型理论和实践

[深度学习] DeepFM 介绍与Pytorch代码解释

https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html

带你认识大模型训练关键算法：分布式训练Allreduce算法
=============================================================================================================================================================
=============================================================================================================================================================
=============================================================================================================================================================
https://www.cnblogs.com/rossiXYZ/p/15943757.html
[源码解析] NVIDIA HugeCTR，GPU 版本参数服务器---(7) ---Distributed Hash之前向传播

3.1 CUB函数
我们首先要介绍几个cub库的方法，这是NVIDIA提供的函数库，用来操作CUDA，把一些常见方法用并行化来实现，比如数组求和，不并行计算就是从头查到尾，如果CUDA并行，则可以高速实现。其网址为：https://docs.nvidia.com/cuda/cub/index.html，配置数据中就采用其中了几个方法。

3.1.1 cub::DeviceScan::InclusiveSum
此函数作用是使用GPU来计算inclusive prefix sum。

使用举例如下：

 * int  num_items;      // e.g., 7
 * int  *d_in;          // e.g., [8, 6, 7, 5, 3, 0, 9]
 * int  *d_out;         // e.g., [ ,  ,  ,  ,  ,  ,  ]
 * ...
 *
 * // Determine temporary device storage requirements for inclusive prefix sum
 * void     *d_temp_storage = NULL;
 * size_t   temp_storage_bytes = 0;
 * cub::DeviceScan::InclusiveSum(d_temp_storage, temp_storage_bytes, d_in, d_out, num_items);
 *
 * // Allocate temporary storage for inclusive prefix sum
 * cudaMalloc(&d_temp_storage, temp_storage_bytes);
 *
 * // Run inclusive prefix sum
 * cub::DeviceScan::InclusiveSum(d_temp_storage, temp_storage_bytes, d_in, d_out, num_items);
 *
 * // d_out <-- [8, 14, 21, 26, 29, 29, 38]
函数实现为：

/**
* \brief Computes a device-wide inclusive prefix sum.
*
* \par
* - Supports non-commutative sum operators.
* - Provides "run-to-run" determinism for pseudo-associative reduction
*   (e.g., addition of floating point types) on the same GPU device.
*   However, results for pseudo-associative reduction may be inconsistent
*   from one device to a another device of a different compute-capability
*   because CUB can employ different tile-sizing for different architectures.
* - \devicestorage
*/
template <
    typename            InputIteratorT,
    typename            OutputIteratorT>
CUB_RUNTIME_FUNCTION
static cudaError_t InclusiveSum(
    void*               d_temp_storage,                 ///< [in] %Device-accessible allocation of temporary storage.  When NULL, the required allocation size is written to \p temp_storage_bytes and no work is done.
    size_t&             temp_storage_bytes,             ///< [in,out] Reference to size in bytes of \p d_temp_storage allocation
    InputIteratorT      d_in,                           ///< [in] Pointer to the input sequence of data items
    OutputIteratorT     d_out,                          ///< [out] Pointer to the output sequence of data items
    int                 num_items,                      ///< [in] Total number of input items (i.e., the length of \p d_in)
    cudaStream_t        stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
    bool                debug_synchronous  = false)     ///< [in] <b>[optional]</b> Whether or not to synchronize the stream after every kernel launch to check for errors.  May cause significant slowdown.  Default is \p false.
{
    // Signed integer type for global offsets
    typedef int OffsetT;

    return DispatchScan<InputIteratorT, OutputIteratorT, Sum, NullType, OffsetT>::Dispatch(
        d_temp_storage,
        temp_storage_bytes,
        d_in,
        d_out,
        Sum(),
        NullType(),
        num_items,
        stream,
        debug_synchronous);
}
如果想深入研究，可以参见 https://nvlabs.github.io/cub/structcub_1_1_device_scan.html 。

3.1.2 cub::DeviceSelect::If
此函数作用是：使用 select_op 函数，将相应的元素从 d_in 分割到一个分区序列 d_out。被复制到第一个分区的元素总数被写入 d_num_selected_out。

具体使用方法为，此例子中，小于7的放在第一个分区，分区内元素数目为5.

 * // Functor type for selecting values less than some criteria
 * struct LessThan
 * {
 *     int compare;
 *
 *     CUB_RUNTIME_FUNCTION __forceinline__
 *     LessThan(int compare) : compare(compare) {}
 *
 *     CUB_RUNTIME_FUNCTION __forceinline__
 *     bool operator()(const int &a) const {
 *         return (a < compare);
 *     }
 * };
 *
 * // Declare, allocate, and initialize device-accessible pointers for input and output
 * int      num_items;              // e.g., 8
 * int      *d_in;                  // e.g., [0, 2, 3, 9, 5, 2, 81, 8]
 * int      *d_out;                 // e.g., [ ,  ,  ,  ,  ,  ,  ,  ]
 * int      *d_num_selected_out;    // e.g., [ ]
 * LessThan select_op(7);
 * ...
 *
 * // Determine temporary device storage requirements
 * void     *d_temp_storage = NULL;
 * size_t   temp_storage_bytes = 0;
 * cub::DeviceSelect::If(d_temp_storage, temp_storage_bytes, d_in, d_out, d_num_selected_out, num_items, select_op);
 *
 * // Allocate temporary storage
 * cudaMalloc(&d_temp_storage, temp_storage_bytes);
 *
 * // Run selection
 * cub::DeviceSelect::If(d_temp_storage, temp_storage_bytes, d_in, d_out, d_num_selected_out, num_items, select_op);
 *
 * // d_out                 <-- [0, 2, 3, 5, 2, 8, 81, 9]
 * // d_num_selected_out    <-- [5]
函数实现是

    /**
     * \brief Uses the \p select_op functor to split the corresponding items from \p d_in into a partitioned sequence \p d_out.  The total number of items copied into the first partition is written to \p d_num_selected_out.
     */
    template <
        typename                    InputIteratorT,
        typename                    OutputIteratorT,
        typename                    NumSelectedIteratorT,
        typename                    SelectOp>
    CUB_RUNTIME_FUNCTION __forceinline__
    static cudaError_t If(
        void*               d_temp_storage,                ///< [in] %Device-accessible allocation of temporary storage.  When NULL, the required allocation size is written to \p temp_storage_bytes and no work is done.
        size_t                      &temp_storage_bytes,            ///< [in,out] Reference to size in bytes of \p d_temp_storage allocation
        InputIteratorT              d_in,                           ///< [in] Pointer to the input sequence of data items
        OutputIteratorT             d_out,                          ///< [out] Pointer to the output sequence of partitioned data items
        NumSelectedIteratorT        d_num_selected_out,             ///< [out] Pointer to the output total number of items selected (i.e., the offset of the unselected partition)
        int                         num_items,                      ///< [in] Total number of items to select from
        SelectOp                    select_op,                      ///< [in] Unary selection operator
        cudaStream_t                stream             = 0,         ///< [in] <b>[optional]</b> CUDA stream to launch kernels within.  Default is stream<sub>0</sub>.
        bool                        debug_synchronous  = false)     ///< [in] <b>[optional]</b> Whether or not to synchronize the stream after every kernel launch to check for errors.  May cause significant slowdown.  Default is \p false.
    {
        typedef int                     OffsetT;         // Signed integer type for global offsets
        typedef NullType*               FlagIterator;   // FlagT iterator type (not used)
        typedef NullType                EqualityOp;     // Equality operator (not used)

        return DispatchSelectIf<InputIteratorT, FlagIterator, OutputIteratorT, NumSelectedIteratorT, SelectOp, EqualityOp, OffsetT, true>::Dispatch(
            d_temp_storage,
            temp_storage_bytes,
            d_in,
            NULL,
            d_out,
            d_num_selected_out,
            select_op,
            EqualityOp(),
            num_items,
            stream,
            debug_synchronous);
    }

};
如果想深入研究，参见 https://nvlabs.github.io/cub/structcub_1_1_device_select.html


0x07 总结
最终结果如下，图有几个被简化的地方，比如hash_table_value_tensors_ 应该是向量的向量，这里简化为向量。
006-015.jpg
embedding vector数值也是虚拟的。嵌入层的最终输出是在 EmbeddingData 的成员变量 train_output_tensors_ 之上。
或者从下面来看。
006-016.jpg

0xFF 参考
快过HugeCTR：用OneFlow轻松实现大型推荐系统引擎

https://nvlabs.github.io/cub/annotated.html

https://developer.nvidia.com/blog/introducing-merlin-hugectr-training-framework-dedicated-to-recommender-systems/

https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/

https://developer.nvidia.com/blog/accelerating-recommender-systems-training-with-nvidia-merlin-open-beta/

HugeCTR源码阅读

embedding层如何反向传播

https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html

稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB

无中生有：论推荐算法中的Embedding思想

tf.nn.embedding_lookup函数原理

求通俗讲解下tensorflow的embedding_lookup接口的意思？

【技术干货】聊聊在大厂推荐场景中embedding都是怎么做的

ctr预估算法对于序列特征embedding可否做拼接，输入MLP?与pooling

推荐系统中的深度匹配模型

土法炮制：Embedding 层是如何实现的？

不等距双杆模型_搜索中的深度匹配模型（下）

深度特征 快牛策略关于高低层特征融合

[深度学习] DeepFM 介绍与Pytorch代码解释

deepFM in pytorch

推荐算法之7——DeepFM模型

DeepFM 参数理解（二）

推荐系统遇上深度学习(三)--DeepFM模型理论和实践

[深度学习] DeepFM 介绍与Pytorch代码解释

https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html

带你认识大模型训练关键算法：分布式训练Allreduce算法

=============================================================================================================================================================
=============================================================================================================================================================
=============================================================================================================================================================

https://www.cnblogs.com/rossiXYZ/p/15965286.html
[源码解析] NVIDIA HugeCTR，GPU 版本参数服务器---(8) ---Distributed Hash之后向传播
0xFF 参考
https://nvlabs.github.io/cub/annotated.html

https://developer.nvidia.com/blog/introducing-merlin-hugectr-training-framework-dedicated-to-recommender-systems/

https://developer.nvidia.com/blog/announcing-nvidia-merlin-application-framework-for-deep-recommender-systems/

https://developer.nvidia.com/blog/accelerating-recommender-systems-training-with-nvidia-merlin-open-beta/

HugeCTR源码阅读

embedding层如何反向传播

https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html

稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB

无中生有：论推荐算法中的Embedding思想

tf.nn.embedding_lookup函数原理

求通俗讲解下tensorflow的embedding_lookup接口的意思？

【技术干货】聊聊在大厂推荐场景中embedding都是怎么做的

ctr预估算法对于序列特征embedding可否做拼接，输入MLP?与pooling

推荐系统中的深度匹配模型

土法炮制：Embedding 层是如何实现的？

不等距双杆模型_搜索中的深度匹配模型（下）

深度特征 快牛策略关于高低层特征融合

[深度学习] DeepFM 介绍与Pytorch代码解释

deepFM in pytorch

推荐算法之7——DeepFM模型

DeepFM 参数理解（二）

推荐系统遇上深度学习(三)--DeepFM模型理论和实践

[深度学习] DeepFM 介绍与Pytorch代码解释

https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html

带你认识大模型训练关键算法：分布式训练Allreduce算法




系列共十篇！
============================================================================================================



https://blog.csdn.net/weixin_42717258/article/details/115643706
HugeCTR源码阅读




http://giantpandacv.com/project/OneFlow/HugeCTR%E6%BA%90%E7%A0%81%E7%AE%80%E5%8D%95%E8%B5%B0%E8%AF%BB_zzk/
https://cloud.tencent.com/developer/article/2011186
HugeCTR 源码简单走读

====版本历史！！！！
PS C:\yk_repo\HugeCTR\HugeCTR> git log --tags --simplify-by-decoration --pretty="format:%ai %d"
2023-08-28 03:44:01 -0700  (tag: v23.08.00)
2023-07-13 17:34:47 -0700  (tag: v23.06.01)
2023-06-14 02:09:38 -0700  (tag: v23.06.00, origin/release-23.06)
2023-05-18 02:24:10 -0700  (tag: v23.05.01)
2023-05-17 21:30:41 -0700  (tag: v23.05.00)
2023-04-20 22:29:26 -0700  (tag: v23.04.00)
2023-03-24 08:14:42 -0700  (origin/release-23.03)
2023-03-06 00:00:42 -0800  (tag: v23.02.00, origin/release-23.02)
2023-02-09 09:37:46 +0800  (tag: v4.3.1)
2023-01-04 18:59:33 -0800  (tag: v4.3)
2022-11-09 17:22:43 -0800  (tag: v4.2)
2022-11-02 01:03:21 -0700  (tag: v4.1.1)
2022-10-17 00:27:20 -0700  (tag: v4.1, origin/gh-pa)
2022-09-14 02:18:36 -0700  (tag: v4.0, origin/page)
2022-09-07 19:22:27 -0700  (tag: v3.9.1)
2022-08-16 08:42:50 -0700  (tag: v3.9)
2022-07-14 09:31:54 +0900  (tag: v3.8)
2022-06-16 12:52:16 +0800  (HEAD -> tag_v3.7, tag: v3.7)
2022-05-11 07:22:19 -0700  (tag: v3.6)
2022-04-01 06:47:42 -0700  (tag: v3.5)
2022-03-01 04:38:28 -0800  (tag: v3.4.1)
2022-01-28 15:41:33 +0800  (tag: v3.4)
2021-12-31 05:33:41 -0800  (tag: v3.3.1)
2021-12-07 03:34:25 -0800  (tag: v3.3)
2021-12-06 06:56:36 -0800  (tag: v3.3_alpha, origin/v3.3-integration)
2021-11-02 05:21:00 -0700  (tag: v3.2.1)
2021-09-22 14:37:38 +0800  (tag: v3.2)
2021-08-03 09:33:19 +0800  (tag: v3.1)
2021-10-08 13:48:04 +0800  (tag: v3.3_beta)
2021-04-11 19:23:46 -0700  (tag: v3.0.1)
2021-03-08 19:18:29 -0800  (tag: v3.0)
2020-11-23 23:15:13 +0900  (tag: v2.3)
2021-05-20 18:14:34 +0200  (tag: v3.1_beta)
2020-09-17 22:21:09 +0800  (tag: v2.2.1)
2020-07-27 06:38:37 +0800  (tag: v2.2)
2020-05-14 16:31:26 +0800  (tag: v2.1)
2020-04-15 04:44:20 -0700  (tag: v2.1_beta, tag: v2.1.0)
2019-09-20 16:57:54 +0900  (tag: v2.0.0)
2020-06-17 11:03:56 -0700  (tag: v2.2-beta, tag: v2.1_a100_update, origin/a100-preview)
2020-06-12 08:07:57 -0700  (tag: v2.1_a100)
2019-09-18 14:37:49 +0900
PS C:\yk_repo\HugeCTR\HugeCTR>